{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from gensim import matutils,corpora, models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have positive reviews and negative reviews in separate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_list = os.listdir(\"negative_reviews\") # names of all files in the negative_polarity dir into a list\n",
    "positive_list = os.listdir(\"positive_reviews\") # names of all files in the positive_polarity dir into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(files_list,root_dir,polarity):\n",
    "    labeled_class = []\n",
    "    reviews = []\n",
    "    actual_class =[]\n",
    "    for j in files_list:\n",
    "        labeled_class.append(polarity)\n",
    "        newj = re.sub(\"^\\.\\_\",\"\",j)\n",
    "#        print(newj)\n",
    "        k = str(open(root_dir + '/' + newj).read())\n",
    "        reviews.append(k)\n",
    "        actual_class.append(str(newj.split('_')[0]))\n",
    "    data = pd.DataFrame({'labeled_class':labeled_class,'review':reviews,'actual_class':actual_class})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df = preprocess(negative_list,'negative_reviews','negative')\n",
    "positive_df = preprocess(positive_list,'positive_reviews','positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have Positive Feedback -> True,Fake Review\n",
    "\n",
    "We have Negative Feedback -> True,Fake Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for i in positive_df.index:\n",
    "    if ((positive_df['labeled_class'][i] == 'positive') & (positive_df['actual_class'][i] == 't')):\n",
    "        target.append(2)\n",
    "    elif ((positive_df['labeled_class'][i] == 'positive') & (positive_df['actual_class'][i] == 'd')):\n",
    "        target.append(1)\n",
    "    else:\n",
    "        print('Error!')\n",
    "positive_df['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "for i in negative_df.index:\n",
    "    if ((negative_df['labeled_class'][i] == 'negative') & (negative_df['actual_class'][i] == 't')):\n",
    "        target.append(3)\n",
    "    elif ((negative_df['labeled_class'][i] == 'negative') & (negative_df['actual_class'][i] == 'd')):\n",
    "        target.append(4)\n",
    "    else:\n",
    "        print('Error!')\n",
    "negative_df['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = positive_df.merge(negative_df,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['review','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import get_data_path\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_tags=[];\n",
    "#g=[[]];\n",
    "#for datapoint in data['review_tokenized']:\n",
    " #   s=\"\"\n",
    "  #  for j in datapoint:\n",
    "   #     s=s+str(j)+\"_\"+j.pos_;\n",
    "    #    s=s+\",\"\n",
    "    #t=s;    \n",
    "    #pos_tags.append(t)  \n",
    "    #g.append(pos_tags)\n",
    "#print(g) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "for doc in nlp.pipe(data['review'].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=6):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append(str([n.text for n in doc ]))\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)   \n",
    "        \n",
    "data['species_tokens'] = tokens\n",
    "data['species_lemma'] = lemma\n",
    "data['species_pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['species_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['species_pos'] = data['species_pos'].astype(str).apply(lambda x: ', '.join(data['species_pos'].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = []\n",
    "for i,j in zip(data['species_tokens'], data['species_pos']):\n",
    "   col.append([ x+'_'+ y for x,y in zip(eval(i),j)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review_tokenized'] = pd.Series(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import matutils,corpora, models\n",
    "\n",
    "def vectorize_comments(df):\n",
    "    d = corpora.Dictionary(df[\"review_tokenized\"])\n",
    "    d.filter_extremes(no_below=3)\n",
    "    d.compactify()\n",
    "    corpus = [d.doc2bow(text) for text in df[\"review_tokenized\"]]\n",
    "    corpus = matutils.corpus2csc(corpus, num_terms=len(d.token2id))\n",
    "    corpus = corpus.transpose()\n",
    "    return d, corpus\n",
    "\n",
    "dictionary,corpus = vectorize_comments(data)\n",
    "print (corpus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X,y):\n",
    "    parameters = {'C': [1000],'random_state':[42]}\n",
    "    clf = GridSearchCV(SVC(), cv=10, param_grid=parameters)\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, data[\"target\"], test_size=0.3, random_state=2016)\n",
    "svc_clf = train_svm(X_train,y_train)\n",
    "svc_clf.fit=(X_train,y_train) \n",
    "print(\"Accuracy of SVM on test sets is : {}\".format(svc_clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "y_pred = svc_clf.predict(X_test)\n",
    "confusion_matrix(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "matthews_corrcoef(y_test, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(svc_clf,corpus, data[\"target\"], cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores) \n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(review):\n",
    "    a = svc_clf.predict(review)\n",
    "    if a == 1.0 :\n",
    "        return('Fake Review (Positive)')\n",
    "    elif a == 2.0:\n",
    "        return('True Review (Positive)')\n",
    "    elif a == 3.0:\n",
    "        return('True Review (Negative)')\n",
    "    else :\n",
    "        return('Fake Review (Negative)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_test:\n",
    "    print(model_test(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
